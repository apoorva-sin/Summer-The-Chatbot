{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chat.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5du6jhcpejL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import csv\n",
        "import random\n",
        "import re\n",
        "import os\n",
        "import unicodedata\n",
        "import codecs\n",
        "import itertools"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfdadr_SrYnH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e70c8773-1a08-4431-b7ee-12dc261a82b7"
      },
      "source": [
        "CUDA = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if CUDA else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 494,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEpY90vBr4-_",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "b4ca7cb0-0f69-4491-9e32-679d22cb3101"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d46e8a00-b61e-4399-873f-42a3562dd533\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-d46e8a00-b61e-4399-873f-42a3562dd533\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving movie_conversations.txt to movie_conversations.txt\n",
            "Saving movie_lines.txt to movie_lines.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5QxoSjNuJfY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import data\n",
        "import io\n",
        "lines_path = 'movie_lines.txt'\n",
        "conv_path = 'movie_conversations.txt'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6zJu9skvD3Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "7e5f74a1-281e-478d-f429-9110e16547a5"
      },
      "source": [
        "# Visualizing the data\n",
        "with open(lines_path, 'r', encoding='iso-8859-1') as file:\n",
        "    lines = file.readlines()\n",
        "for line in lines[0:8]:\n",
        "    print(line.strip())"
      ],
      "execution_count": 495,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!\n",
            "L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!\n",
            "L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.\n",
            "L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?\n",
            "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go.\n",
            "L924 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ Wow\n",
            "L872 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Okay -- you're gonna need to learn how to lie.\n",
            "L871 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ No\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM42jSVOvGMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data preparation Part 1\n",
        "\"\"\" Create a dictionary of the form LineID: {LineID : L1\n",
        "                                             CharID : C1 \n",
        "                                             .\n",
        "                                             .\n",
        "                                             Text : \"Something\"} \"\"\"\n",
        "\n",
        "line_fields = [\"lineID\", \"characterID\", \"movieID\", \"character\", \"text\"]\n",
        "lines = {}\n",
        "\n",
        "with open(lines_path, 'r', encoding='iso-8859-1') as f:\n",
        "    for line in f:\n",
        "        values = line.split(\" +++$+++ \")\n",
        "\n",
        "        line_object = {}\n",
        "        \n",
        "        for i,field in enumerate(line_fields):\n",
        "            line_object[field] = values[i]\n",
        "        lines[line_object[\"lineID\"]] = line_object"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V51guYl85shU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9db77702-f649-4b4d-e6f8-f119938ec35f"
      },
      "source": [
        "lines['L1000']"
      ],
      "execution_count": 497,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'character': 'WALTER',\n",
              " 'characterID': 'u11',\n",
              " 'lineID': 'L1000',\n",
              " 'movieID': 'm0',\n",
              " 'text': \"Oh, Christ.  Don't tell me you've changed your mind.  I already sent 'em a check.\\n\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 497
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gLcUDJxX5zYc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data preparation Part 2\n",
        "\n",
        "conv_fields = [\"character1ID\", \"character2ID\", \"movieID\", \"utteranceIDs\"] # c1 and c2 talking in movie M with dialogues - l1,l2,l3,l4 (lineIDs)\n",
        "conversations = []\n",
        "\n",
        "with open(conv_path, 'r', encoding='iso-8859-1') as f:\n",
        "    for line in f:\n",
        "        values = line.split(\" +++$+++ \")\n",
        "\n",
        "        conv_object = {}\n",
        "        \n",
        "        for i,field in enumerate(conv_fields):\n",
        "            conv_object[field] = values[i]\n",
        "        lineIDs = eval(conv_object[\"utteranceIDs\"])\n",
        "\n",
        "        conv_object[\"lines\"] = []\n",
        "\n",
        "        for lineID in lineIDs:\n",
        "            conv_object[\"lines\"].append(lines[lineID])\n",
        "        conversations.append(conv_object)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyfuyCuYCx2x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "ad09cdd0-9c8e-45ac-99db-4c051c174123"
      },
      "source": [
        "conversations[1]"
      ],
      "execution_count": 499,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'character1ID': 'u0',\n",
              " 'character2ID': 'u2',\n",
              " 'lines': [{'character': 'BIANCA',\n",
              "   'characterID': 'u0',\n",
              "   'lineID': 'L198',\n",
              "   'movieID': 'm0',\n",
              "   'text': \"You're asking me out.  That's so cute. What's your name again?\\n\"},\n",
              "  {'character': 'CAMERON',\n",
              "   'characterID': 'u2',\n",
              "   'lineID': 'L199',\n",
              "   'movieID': 'm0',\n",
              "   'text': 'Forget it.\\n'}],\n",
              " 'movieID': 'm0',\n",
              " 'utteranceIDs': \"['L198', 'L199']\\n\"}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 499
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRK-RWpRDXOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Extract pairs of sentences from conversations\n",
        "qa_pairs = []\n",
        "\n",
        "for conversation in conversations:\n",
        "    for i in range(len(conversation[\"lines\"]) - 1):\n",
        "        input_line = conversation[\"lines\"][i][\"text\"].strip()\n",
        "        target_line = conversation[\"lines\"][i+1][\"text\"].strip()\n",
        "        if input_line and target_line:\n",
        "            qa_pairs.append([input_line, target_line])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYK36cRfG-9m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "6c05dbad-57b6-4d36-e67c-3d6fcd6dc456"
      },
      "source": [
        "qa_pairs[0:10]"
      ],
      "execution_count": 501,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.',\n",
              "  \"Well, I thought we'd start with pronunciation, if that's okay with you.\"],\n",
              " [\"Well, I thought we'd start with pronunciation, if that's okay with you.\",\n",
              "  'Not the hacking and gagging and spitting part.  Please.'],\n",
              " ['Not the hacking and gagging and spitting part.  Please.',\n",
              "  \"Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\"],\n",
              " [\"You're asking me out.  That's so cute. What's your name again?\",\n",
              "  'Forget it.'],\n",
              " [\"No, no, it's my fault -- we didn't have a proper introduction ---\",\n",
              "  'Cameron.'],\n",
              " ['Cameron.',\n",
              "  \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\"],\n",
              " [\"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\",\n",
              "  'Seems like she could get a date easy enough...'],\n",
              " ['Why?',\n",
              "  'Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.'],\n",
              " ['Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.',\n",
              "  \"That's a shame.\"],\n",
              " ['Gosh, if only we could find Kat a boyfriend...',\n",
              "  'Let me see what I can do.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 501
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C7_67rM4HUdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the formatted conversation pairs\n",
        "with open(\"formattedLines.txt\", 'w', encoding='utf-8') as outputfile:\n",
        "    writer = csv.writer(outputfile, delimiter = str(codecs.decode('\\t',\"unicode_escape\")))\n",
        "    for pair in qa_pairs:\n",
        "        writer.writerow(pair)\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFh7p9MMKjD4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "ae17ca55-0959-4e49-9839-6e0c574c09ee"
      },
      "source": [
        "# Download the saved file\n",
        "files.download('formattedLines.txt')"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-102-643a9d037d4e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'formattedLines.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    176\u001b[0m       \u001b[0;34m'port'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m       \u001b[0;34m'path'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m       \u001b[0;34m'name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m   })\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB6oWmllKpvx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "9d261439-0ade-4ad9-b763-afc8d466676a"
      },
      "source": [
        "# Loading in the saved formatted file\n",
        "with open(\"formattedLines.txt\",'r') as file:\n",
        "    lines = file.readlines()\n",
        "for line in lines[0:8]:\n",
        "    print(line)"
      ],
      "execution_count": 503,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\tWell, I thought we'd start with pronunciation, if that's okay with you.\n",
            "\n",
            "Well, I thought we'd start with pronunciation, if that's okay with you.\tNot the hacking and gagging and spitting part.  Please.\n",
            "\n",
            "Not the hacking and gagging and spitting part.  Please.\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
            "\n",
            "You're asking me out.  That's so cute. What's your name again?\tForget it.\n",
            "\n",
            "No, no, it's my fault -- we didn't have a proper introduction ---\tCameron.\n",
            "\n",
            "Cameron.\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
            "\n",
            "The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\tSeems like she could get a date easy enough...\n",
            "\n",
            "Why?\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aa0wOcyGoU8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Word Processing and vocabulary\n",
        "PAD_TOKEN = 0\n",
        "SOS_TOKEN = 1\n",
        "EOS_TOKEN = 2\n",
        "\n",
        "class Vocabulary():\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_TOKEN: \"PAD\", SOS_TOKEN: \"SOS\", EOS_TOKEN: \"EOS\"}\n",
        "        self.numwords = 3\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.numwords\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.numwords] = word\n",
        "            self.numwords += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    # Trim to remove words below a certain threshold, so that the network doesn't get confusedddd.\n",
        "\n",
        "    def trim(self, min_count):\n",
        "        keep_words = []\n",
        "        for k,v in self.word2count.items():\n",
        "            if v>=min_count:\n",
        "                keep_words.append(k)\n",
        "\n",
        "        # Reinitialize the dictionaries with the new set of words to be kept\n",
        "\n",
        "        print(\"Keep words {} / {} = {:.4f}\".format(len(keep_words), len(self.word2index), len(keep_words)/len(self.word2index)))\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_TOKEN: \"PAD\", SOS_TOKEN: \"SOS\", EOS_TOKEN: \"EOS\"}\n",
        "        self.numwords = 3\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLtAB_MBtKDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unicodeToAscii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD',s) if unicodedata.category(c)!='Mn')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTZrApOltxUB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8d0a2907-dbdf-4917-9aa6-b515a08ad5f6"
      },
      "source": [
        "unicodeToAscii('Montr√©al')"
      ],
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Montreal'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 230
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEsKr5bGt5EX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2JdCIM6weff",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f9d9e4d-a222-48f0-fb6e-de58b807ea09"
      },
      "source": [
        "normalizeString(\"aa123aa!s's   dd?\")"
      ],
      "execution_count": 232,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'aa aa !s s dd ?'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 232
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5zdok9r8NTF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "outputId": "5cb85014-a4be-4493-a3cd-86fffbdfc20f"
      },
      "source": [
        "lines[0:10]"
      ],
      "execution_count": 233,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\\tWell, I thought we'd start with pronunciation, if that's okay with you.\\n\",\n",
              " \"Well, I thought we'd start with pronunciation, if that's okay with you.\\tNot the hacking and gagging and spitting part.  Please.\\n\",\n",
              " \"Not the hacking and gagging and spitting part.  Please.\\tOkay... then how 'bout we try out some French cuisine.  Saturday?  Night?\\n\",\n",
              " \"You're asking me out.  That's so cute. What's your name again?\\tForget it.\\n\",\n",
              " \"No, no, it's my fault -- we didn't have a proper introduction ---\\tCameron.\\n\",\n",
              " \"Cameron.\\tThe thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\n\",\n",
              " \"The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\\tSeems like she could get a date easy enough...\\n\",\n",
              " 'Why?\\tUnsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\n',\n",
              " \"Unsolved mystery.  She used to be really popular when she started high school, then it was just like she got sick of it or something.\\tThat's a shame.\\n\",\n",
              " 'Gosh, if only we could find Kat a boyfriend...\\tLet me see what I can do.\\n']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 233
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxy4Q6LTwmc5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "09b4856f-0ef3-4cb4-f49c-d9751c75becc"
      },
      "source": [
        "# Opening up the formatted text file\n",
        "print(\"Reading and processing text... Please wait.\")\n",
        "lines = open(\"formattedLines.txt\", encoding='utf-8').read().strip().split('\\n') # Lines is gonna become a list of lines \n",
        "                                                                                 # where each line is a pair of dialogues\n",
        "pairs = [[normalizeString(s) for s in pair.split('\\t')] for pair in lines]\n",
        "print(\"Processing complete!\")"
      ],
      "execution_count": 234,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading and processing text... Please wait.\n",
            "Processing complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZnCLAkq7xx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = Vocabulary(\"Cornell\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GebDIKM_x7Qc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "959d5a4a-fda0-4dc9-aec3-a4d3c7f9d6c0"
      },
      "source": [
        "pairs[0]"
      ],
      "execution_count": 236,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['can we make this quick ? roxanne korrine and andrew barrett are having an incredibly horrendous public break up on the quad . again .',\n",
              " 'well i thought we d start with pronunciation if that s okay with you .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 236
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5pUfpFTM1cWT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#### Further processing the text ####\n",
        "\n",
        "# Keep only sentences <= MAX_LEN\n",
        "\n",
        "MAX_LEN = 10\n",
        "def filterPair(p):\n",
        "    return len(p[0].split())<MAX_LEN and len(p[1].split())<MAX_LEN\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuIMMeAi4GQ-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ac66c113-ce38-4892-b47d-2f26250acf3f"
      },
      "source": [
        "pairs = filterPairs(pairs)\n",
        "print(\"Filtered Pairs = {}\".format(len(pairs)))"
      ],
      "execution_count": 238,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Filtered Pairs = 64271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzR-lBWM5DCc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4abf6957-636e-4939-8651-ba20c9765723"
      },
      "source": [
        "# Add the question and reply sentence pairs to the vocabulary\n",
        "for pair in pairs:\n",
        "    vocab.addSentence(pair[0])\n",
        "    vocab.addSentence(pair[1])\n",
        "print(\"Unique word count = {}\".format(vocab.numwords))"
      ],
      "execution_count": 239,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique word count = 18008\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgn1zJAE6aA5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b7eef55e-daa0-4d97-ee19-562de06cfd35"
      },
      "source": [
        "for pair in pairs[0:10]:\n",
        "    print(pair)"
      ],
      "execution_count": 240,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['there .', 'where ?']\n",
            "['you have my word . as a gentleman', 'you re sweet .']\n",
            "['hi .', 'looks like things worked out tonight huh ?']\n",
            "['you know chastity ?', 'i believe we share an art instructor']\n",
            "['have fun tonight ?', 'tons']\n",
            "['well no . . .', 'then that s all you had to say .']\n",
            "['then that s all you had to say .', 'but']\n",
            "['but', 'you always been this selfish ?']\n",
            "['do you listen to this crap ?', 'what crap ?']\n",
            "['what good stuff ?', 'the real you .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYCcGl508Civ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "44facf9a-2f94-42d5-8aaf-87dda5e6503b"
      },
      "source": [
        "# Keep only those words which occur more than MIN_COUNT times :)\n",
        "\n",
        "MIN_COUNT = 3\n",
        "\n",
        "def trimRareWords(vocab, pairs, MIN_COUNT):\n",
        "    vocab.trim(MIN_COUNT)\n",
        "\n",
        "    keep_pairs = []\n",
        "\n",
        "    for pair in pairs:\n",
        "        input_sentence = pair[0]\n",
        "        output_sentence = pair[1]\n",
        "\n",
        "        keep_input = True\n",
        "        keep_output = True\n",
        "    \n",
        "        for word in input_sentence.split(' '):\n",
        "            if word not in vocab.word2index:\n",
        "                keep_input = False\n",
        "                break\n",
        "                \n",
        "        for word in output_sentence.split(' '):\n",
        "            if word not in vocab.word2index:\n",
        "                keep_output = False\n",
        "                break\n",
        "\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {} pairs, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs)/len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "pairs = trimRareWords(vocab, pairs, MIN_COUNT)\n"
      ],
      "execution_count": 241,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Keep words 7823 / 18005 = 0.4345\n",
            "Trimmed from 64271 pairs to 53165 pairs, 0.8272 of total\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdZX7pZK_sQT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################### Data Preparation ##########################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA1NnzTxBrou",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Encode a sentence into a list of integers\n",
        "\n",
        "def indexesFromSentence(vocab, sentence):\n",
        "    return [vocab.word2index[word] for word in sentence.split(' ')] + [EOS_TOKEN]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3cE2F9jGGam",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "dd2d8c8e-aad8-4673-e4e1-3dbe9256eb48"
      },
      "source": [
        "print(pairs[1][0])\n",
        "indexesFromSentence(vocab, pairs[1][0])"
      ],
      "execution_count": 244,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "you have my word . as a gentleman\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[7, 8, 9, 10, 4, 11, 12, 13, 2]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 244
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgkxlSb_GMVw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "outputId": "d511edab-ff94-487f-d458-5771a1c08a46"
      },
      "source": [
        "# Padding the sentences for equal length\n",
        "def zeroPadding(l, fillvalue = 0):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "# Defining a sample for testing\n",
        "inp = []\n",
        "out = []\n",
        "for pair in pairs[:10]:\n",
        "    inp.append(pair[0])\n",
        "    out.append(pair[1])\n",
        "print(inp)\n",
        "print(len(inp))\n",
        "indexes = [indexesFromSentence(vocab, sentence) for sentence in inp]\n",
        "indexes"
      ],
      "execution_count": 245,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['there .', 'you have my word . as a gentleman', 'hi .', 'have fun tonight ?', 'well no . . .', 'then that s all you had to say .', 'but', 'do you listen to this crap ?', 'what good stuff ?', 'wow']\n",
            "10\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3, 4, 2],\n",
              " [7, 8, 9, 10, 4, 11, 12, 13, 2],\n",
              " [16, 4, 2],\n",
              " [8, 31, 22, 6, 2],\n",
              " [33, 34, 4, 4, 4, 2],\n",
              " [35, 36, 37, 38, 7, 39, 40, 41, 4, 2],\n",
              " [42, 2],\n",
              " [47, 7, 48, 40, 45, 49, 6, 2],\n",
              " [50, 51, 52, 6, 2],\n",
              " [58, 2]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 245
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XwGwOCxuJ2Pg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "173d1c8d-d166-4f59-85a4-6947164e12f7"
      },
      "source": [
        "leng = [len(ind) for ind in indexes]\n",
        "max(leng)"
      ],
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 246
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odY_Fn55L8Eq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "2c09e863-ca2f-48cf-9ca1-8951a847855c"
      },
      "source": [
        "# Prints maximum length (sentence length) x no. of sentences in a batch (batch size) matrix\n",
        "# Each column is a sentence\n",
        "test_result = zeroPadding(indexes)\n",
        "test_result"
      ],
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(3, 7, 16, 8, 33, 35, 42, 47, 50, 58),\n",
              " (4, 8, 4, 31, 34, 36, 2, 7, 51, 2),\n",
              " (2, 9, 2, 22, 4, 37, 0, 48, 52, 0),\n",
              " (0, 10, 0, 6, 4, 38, 0, 40, 6, 0),\n",
              " (0, 4, 0, 2, 4, 7, 0, 45, 2, 0),\n",
              " (0, 11, 0, 0, 2, 39, 0, 49, 0, 0),\n",
              " (0, 12, 0, 0, 0, 40, 0, 6, 0, 0),\n",
              " (0, 13, 0, 0, 0, 41, 0, 2, 0, 0),\n",
              " (0, 2, 0, 0, 0, 4, 0, 0, 0, 0),\n",
              " (0, 0, 0, 0, 0, 2, 0, 0, 0, 0)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 247
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIy_7CDrMD1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def binaryMatrix(l, value=0):\n",
        "    m = []\n",
        "    for i,seq in enumerate(l): # l is a list of lists just like above\n",
        "        m.append([])\n",
        "        for token in seq:\n",
        "            if token == PAD_TOKEN:\n",
        "                m[i].append(0)\n",
        "            else:\n",
        "                m[i].append(1)\n",
        "    return m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmLOWwU5OZvz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "269b3582-b3a8-4ed0-9f53-4bcf9d76815f"
      },
      "source": [
        "binaryResult = binaryMatrix(test_result)\n",
        "binaryResult"
      ],
      "execution_count": 249,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
              " [1, 1, 1, 1, 1, 1, 0, 1, 1, 0],\n",
              " [0, 1, 0, 1, 1, 1, 0, 1, 1, 0],\n",
              " [0, 1, 0, 1, 1, 1, 0, 1, 1, 0],\n",
              " [0, 1, 0, 0, 1, 1, 0, 1, 0, 0],\n",
              " [0, 1, 0, 0, 0, 1, 0, 1, 0, 0],\n",
              " [0, 1, 0, 0, 0, 1, 0, 1, 0, 0],\n",
              " [0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 249
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLH5wGRsOhoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FINALLY DOING SHIT ON ACTUAL DATA omg\n",
        "\n",
        "def inputVar(l, vocab):\n",
        "    indexes_batch = [indexesFromSentence(vocab, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "def outputVar(l, vocab):\n",
        "    indexes_batch = [indexesFromSentence(vocab, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0372rFKSbsk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def batch2TrainData(vocab, pair_batch):\n",
        "    # Sort by QUESTION LENGTH in DESCENDING order\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(' ')), reverse=True)\n",
        "    input_batch, output_batch = [], []\n",
        "    for pair in pair_batch:\n",
        "        input_batch.append(pair[0])\n",
        "        output_batch.append(pair[1])\n",
        "    inp, lengths = inputVar(input_batch, vocab)\n",
        "    out, mask, max_target_len = outputVar(output_batch, vocab)\n",
        "    return inp, lengths, out, mask, max_target_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkyOo_y9V5f0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 612
        },
        "outputId": "f1149c22-feb8-4a90-8164-d12726a84e5b"
      },
      "source": [
        "# Example \n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(vocab, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "print(\"Input Variable:\")\n",
        "print(input_variable)\n",
        "print(\"Lengths of each sentence:\")\n",
        "print(lengths)\n",
        "print(\"Target Variable:\")\n",
        "print(target_variable)\n",
        "print(\"Mask:\")\n",
        "print(mask)\n",
        "print(\"Max target length : \", max_target_len)"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Variable:\n",
            "tensor([[ 270,   51,   50,  124,   23],\n",
            "        [  83, 5060,  368,  601,    6],\n",
            "        [   6, 5061,   40,    4,    2],\n",
            "        [ 379,   12,   53,    4,    0],\n",
            "        [  25,   51, 6153,    4,    0],\n",
            "        [  41, 3426, 2741,    2,    0],\n",
            "        [ 319,    4,    6,    0,    0],\n",
            "        [   6,    2,    2,    0,    0],\n",
            "        [   2,    0,    0,    0,    0]])\n",
            "Lengths of each sentence:\n",
            "tensor([9, 8, 8, 6, 3])\n",
            "Target Variable:\n",
            "tensor([[  56,   25,  467,  598,  716],\n",
            "        [ 827,  450,    4, 4065,  109],\n",
            "        [   4,   24,    2, 1204,  460],\n",
            "        [   2,   86,    0,    4,  111],\n",
            "        [   0,   56,    0,    2,    6],\n",
            "        [   0,    7,    0,    0,    2],\n",
            "        [   0,    9,    0,    0,    0],\n",
            "        [   0, 1082,    0,    0,    0],\n",
            "        [   0,    4,    0,    0,    0],\n",
            "        [   0,    2,    0,    0,    0]])\n",
            "Mask:\n",
            "tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True, False,  True,  True],\n",
            "        [False,  True, False,  True,  True],\n",
            "        [False,  True, False, False,  True],\n",
            "        [False,  True, False, False, False],\n",
            "        [False,  True, False, False, False],\n",
            "        [False,  True, False, False, False],\n",
            "        [False,  True, False, False, False]])\n",
            "Max target length :  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5aYwJWFYCRY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################### DEFINING THE MODEL ##################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDumLc5AaGL4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size # No. of neurons in the hidden layer (NOT timesteps)\n",
        "        self.embedding = embedding\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout), bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "\n",
        "        embedded = self.embedding(input_seq)\n",
        "\n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths) \n",
        "        # Jitne bhi saare words hain unko ek line mein kr diya\n",
        "        # Now we have the batch lengths that are supposed to go into each lstm time-step, toh pick out utne words from the list and put it\n",
        "        # in the lstm. Fir next batch length ke hisaab se next words uthao. So suppose its 6 5 3, total 14 words, toh first choose 6 for the\n",
        "        # first lstm then 5 for the next then 3. \n",
        "        # packed[0] will be 14 fir and packed[1] will be tensor([6,5,3])\n",
        "\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "\n",
        "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "\n",
        "        outputs = outputs[:,:,:self.hidden_size] + outputs[:,:,self.hidden_size:]\n",
        "\n",
        "        return outputs, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LRV-Un5ofQk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attention, self).__init__()\n",
        "        \n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output): # Hidden is the hidden state from the decoder\n",
        "            return torch.sum(hidden*encoder_output, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_output):\n",
        "\n",
        "        # hidden ka shape = (1, batch_size, hidden_size) cause we feed 1 batch row at a time to the GRU cell in the decoder\n",
        "        # ie one row = one cell = one time step. Fir next row goes into the next time step, but it happens one at a time.\n",
        "        # encoder_output ka shape = (max_seq_len, batch_size, hidden_size)\n",
        "        # Multiply krne ke baad shape = Max_length x batch_size x hidden_size\n",
        "        # This is summed across dim=2, ie hidden size hmmmmmmmmmmmm\n",
        "        attention_energies = self.dot_score(hidden, encoder_output) # Max_length x batch_size\n",
        "        attention_energies = attention_energies.t() # Transpose\n",
        "\n",
        "        return F.softmax(attention_energies, dim=1).unsqueeze(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "95FZ660C9SU7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, attention_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.attention_model = attention_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers==1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size*2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attention = Attention(attention_model, hidden_size)\n",
        "    \n",
        "    def forward(self, input_step, last_hidden, encoder_output):\n",
        "        # Input step = (1, batch_size), cause one row of words (one batch) picked up from the array of sentence length x batch size\n",
        "        # Last hidden is the final hidden state of the encoder GRU (n_layers x directions, batch size, hidden size)\n",
        "        # encoder output is the output of the encoder(full memory) (sentence len, batch size, directions x hidden size)\n",
        "        # We run this one step (one batch of words) at a time\n",
        "        \n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden) # RNN_Output = (1, batch size, hidden size x directions)\n",
        "                                                             # Hidden state = (n_layers x directions, batch size, hidden size)\n",
        "\n",
        "        # Attention forward function returns softmax in the form (batch size, 1, max length)\n",
        "        attention_weights = self.attention(rnn_output, encoder_output)\n",
        "\n",
        "        # For the context vector, or what to focus on vector, we multiply the attention with the encoder output\n",
        "        # Attention (batch size, 1, max length) x Encoder output transpose (batch size, max length, hidden size) = (batch size, 1, hidden size)\n",
        "        context = attention_weights.bmm(encoder_output.transpose(0,1))\n",
        "\n",
        "        # Concatenate context with GRU output\n",
        "        rnn_output = rnn_output.squeeze(0) # Remove the 1 from that 3-D tensor to make it 2-D\n",
        "        context = context.squeeze(1) # Both of these are now batch size x hidden size 2-D tensors\n",
        "        concat_input = torch.cat((rnn_output, context),1) # Concatenate along columns, so new size = (batch size, hidden size x 2)\n",
        "        concat_output = torch.tanh(self.concat(concat_input)) # Pass the concat through a linear layer\n",
        "\n",
        "        output = self.out(concat_output) # Size now is batch size x vocab size\n",
        "        output = F.softmax(output, dim=1) # Each batch row contains the probabilities of all the words, so softmax across them to get \n",
        "                                          # the MOST PROBABLE WORD\n",
        "        return output, hidden\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgWGvzAmL3wL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def maskNLLLoss(decoder_out, target, mask): # To NOT calculate loss for padded spaces\n",
        "    nTotal = mask.sum() # Number of elements to consider\n",
        "    target = target.view(-1,1)\n",
        "\n",
        "    gathered_tensor = torch.gather(decoder_out, 1, target)\n",
        "\n",
        "    crossEntropy = -torch.log(gathered_tensor) # Calculate the loss on the gathered tensor\n",
        "\n",
        "    loss = crossEntropy.masked_select(mask)\n",
        "    loss = loss.mean()\n",
        "    loss = loss.to(device)\n",
        "\n",
        "    return loss, nTotal.item()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE1frD9TQXX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "################################################################# TRAINING ########################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCBr9XOZVePo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9afe0cbd-f243-4994-bf5e-cddfa4e8a888"
      },
      "source": [
        "#<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<< This is only for visualization >>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
        "small_batch_size = 5\n",
        "batches = batch2TrainData(vocab, [random.choice(pairs) for _ in range(small_batch_size)])\n",
        "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
        "\n",
        "### One time step is one batch of words ###\n",
        "\n",
        "print(\"Input Variable:\")\n",
        "print(input_variable)\n",
        "print(\"Lengths of each sentence:\")\n",
        "print(lengths)\n",
        "print(\"Target Variable:\")\n",
        "print(target_variable)\n",
        "print(\"Mask:\")\n",
        "print(mask)\n",
        "\n",
        "print(\"Input Variable Shape:\")\n",
        "print(input_variable.shape)\n",
        "print(\"Lengths Shape:\")\n",
        "print(lengths.shape)\n",
        "print(\"Target Variable Shape:\")\n",
        "print(target_variable.shape)\n",
        "print(\"Mask Shape:\")\n",
        "print(mask.shape)\n",
        "print(\"Max target length : \", max_target_len)\n",
        "\n",
        "# Defining the parameters\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "attention_model = 'dot'\n",
        "embedding = nn.Embedding(vocab.numwords, hidden_size)\n",
        "\n",
        "# Defining the encoder and decoder\n",
        "encoder = Encoder(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = Decoder(attention_model, embedding, hidden_size, vocab.numwords, decoder_n_layers,dropout)\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=0.0001) # Parameters() specifies the weights of the encoder/decoder for the optimizer\n",
        "                                                               # to differentiate and subtract from and do whatever with\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=0.0001)\n",
        "encoder_optimizer.zero_grad()\n",
        "decoder_optimizer.zero_grad()\n",
        "\n",
        "input_variable = input_variable.to(device)\n",
        "lengths = lengths.to(device)\n",
        "target_variable = target_variable.to(device)\n",
        "mask = mask.to(device)\n",
        "\n",
        "loss = 0\n",
        "print_losses = []\n",
        "n_totals = 0\n",
        "\n",
        "encoder_output, encoder_hidden = encoder(input_variable, lengths)\n",
        "print(\"Encoder Output Shape = \",encoder_output.shape)\n",
        "print(\"Last Encoder Hidden State Shape = \",encoder_hidden.shape)\n",
        "\n",
        "decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(small_batch_size)]])\n",
        "decoder_input = decoder_input.to(device)\n",
        "print(\"Initial Decoder Input Shape = \",decoder_input.shape)\n",
        "print(decoder_input)\n",
        "\n",
        "# Last encoder hidden state is passed to the decoder as the initial hidden state\n",
        "decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "print(\"Initial decoder hidden state shape = \",decoder_hidden.shape)\n",
        "print(\"\\n\")\n",
        "print(\"----------------------------------------------------------\")\n",
        "print(\"THIS IS WHAT HAPPENS AT EVERY TIME STEP OF THE GRU!\")\n",
        "print(\"----------------------------------------------------------\")\n",
        "print(\"\\n\")\n",
        "\n",
        "for t in range(max_target_len):\n",
        "    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "    print(\"Decoder Output Shape = \", decoder_output.shape)    \n",
        "    print(\"Decoder Hidden State Shape = \", decoder_hidden.shape)\n",
        "\n",
        "    decoder_input = target_variable[t].view(1,-1) # Cause Teacher Forcing\n",
        "    print(\"Target Variable now = \", target_variable[t])\n",
        "    print(\"Target Variable Shape now = \", target_variable[t].shape)\n",
        "    print(\"Decoder input shape after reshaping = \", decoder_input.shape)\n",
        "\n",
        "    # Loss\n",
        "    print(\"Mask for current timestep\", mask[t])\n",
        "    print(\"Mask shape for current timestep\", mask[t].shape)\n",
        "    mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "    print(\"Mask Loss = \", mask_loss)\n",
        "    print(\"Total = \", nTotal)\n",
        "\n",
        "    loss += mask_loss\n",
        "    print_losses.append(mask_loss.item()*nTotal)\n",
        "    print(print_losses)\n",
        "    n_totals += nTotal\n",
        "    print(nTotal)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    returned_loss = sum(print_losses)/n_totals\n",
        "    print(\"Returned Loss = \", returned_loss)\n",
        "    print(\"\\n\")\n",
        "    print(\"----------------------------------------DONE ONE STEP-----------------------------------\")\n",
        "    print(\"\\n\")\n",
        " "
      ],
      "execution_count": 469,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Variable:\n",
            "tensor([[  42,   50,  252,   34,  318],\n",
            "        [  76,   25,  387, 4160,    7],\n",
            "        [  37,   47,   25,  916,   94],\n",
            "        [  67,   40,    4,    4,    4],\n",
            "        [ 325, 5640,    2,    2,    2],\n",
            "        [ 115,    6,    0,    0,    0],\n",
            "        [  76,    2,    0,    0,    0],\n",
            "        [   6,    0,    0,    0,    0],\n",
            "        [   2,    0,    0,    0,    0]])\n",
            "Lengths of each sentence:\n",
            "tensor([9, 7, 5, 5, 5])\n",
            "Target Variable:\n",
            "tensor([[  50,  318,   62,    7,  383],\n",
            "        [   6,  614,   50,  389,    7],\n",
            "        [   2,   83,   94, 4012,    4],\n",
            "        [   0, 5640,   27,  276,    2],\n",
            "        [   0,    4, 1589,    4,    0],\n",
            "        [   0,    2,    6,    2,    0],\n",
            "        [   0,    0,    2,    0,    0]])\n",
            "Mask:\n",
            "tensor([[ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True],\n",
            "        [False,  True,  True,  True,  True],\n",
            "        [False,  True,  True,  True, False],\n",
            "        [False,  True,  True,  True, False],\n",
            "        [False, False,  True, False, False]])\n",
            "Input Variable Shape:\n",
            "torch.Size([9, 5])\n",
            "Lengths Shape:\n",
            "torch.Size([5])\n",
            "Target Variable Shape:\n",
            "torch.Size([7, 5])\n",
            "Mask Shape:\n",
            "torch.Size([7, 5])\n",
            "Max target length :  7\n",
            "Encoder Output Shape =  torch.Size([9, 5, 500])\n",
            "Last Encoder Hidden State Shape =  torch.Size([4, 5, 500])\n",
            "Initial Decoder Input Shape =  torch.Size([1, 5])\n",
            "tensor([[1, 1, 1, 1, 1]], device='cuda:0')\n",
            "Initial decoder hidden state shape =  torch.Size([2, 5, 500])\n",
            "\n",
            "\n",
            "----------------------------------------------------------\n",
            "THIS IS WHAT HAPPENS AT EVERY TIME STEP OF THE GRU!\n",
            "----------------------------------------------------------\n",
            "\n",
            "\n",
            "Decoder Output Shape =  torch.Size([5, 7826])\n",
            "Decoder Hidden State Shape =  torch.Size([2, 5, 500])\n",
            "Target Variable now =  tensor([ 50, 318,  62,   7, 383], device='cuda:0')\n",
            "Target Variable Shape now =  torch.Size([5])\n",
            "Decoder input shape after reshaping =  torch.Size([1, 5])\n",
            "Mask for current timestep tensor([True, True, True, True, True], device='cuda:0')\n",
            "Mask shape for current timestep torch.Size([5])\n",
            "Mask Loss =  tensor(9.0255, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Total =  5\n",
            "[45.12771129608154]\n",
            "5\n",
            "Returned Loss =  9.025542259216309\n",
            "\n",
            "\n",
            "----------------------------------------DONE ONE STEP-----------------------------------\n",
            "\n",
            "\n",
            "Decoder Output Shape =  torch.Size([5, 7826])\n",
            "Decoder Hidden State Shape =  torch.Size([2, 5, 500])\n",
            "Target Variable now =  tensor([  6, 614,  50, 389,   7], device='cuda:0')\n",
            "Target Variable Shape now =  torch.Size([5])\n",
            "Decoder input shape after reshaping =  torch.Size([1, 5])\n",
            "Mask for current timestep tensor([True, True, True, True, True], device='cuda:0')\n",
            "Mask shape for current timestep torch.Size([5])\n",
            "Mask Loss =  tensor(9.0222, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Total =  5\n",
            "[45.12771129608154, 45.110840797424316]\n",
            "5\n",
            "Returned Loss =  9.023855209350586\n",
            "\n",
            "\n",
            "----------------------------------------DONE ONE STEP-----------------------------------\n",
            "\n",
            "\n",
            "Decoder Output Shape =  torch.Size([5, 7826])\n",
            "Decoder Hidden State Shape =  torch.Size([2, 5, 500])\n",
            "Target Variable now =  tensor([   2,   83,   94, 4012,    4], device='cuda:0')\n",
            "Target Variable Shape now =  torch.Size([5])\n",
            "Decoder input shape after reshaping =  torch.Size([1, 5])\n",
            "Mask for current timestep tensor([True, True, True, True, True], device='cuda:0')\n",
            "Mask shape for current timestep torch.Size([5])\n",
            "Mask Loss =  tensor(8.9643, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Total =  5\n",
            "[45.12771129608154, 45.110840797424316, 44.82134819030762]\n",
            "5\n",
            "Returned Loss =  9.003993352254232\n",
            "\n",
            "\n",
            "----------------------------------------DONE ONE STEP-----------------------------------\n",
            "\n",
            "\n",
            "Decoder Output Shape =  torch.Size([5, 7826])\n",
            "Decoder Hidden State Shape =  torch.Size([2, 5, 500])\n",
            "Target Variable now =  tensor([   0, 5640,   27,  276,    2], device='cuda:0')\n",
            "Target Variable Shape now =  torch.Size([5])\n",
            "Decoder input shape after reshaping =  torch.Size([1, 5])\n",
            "Mask for current timestep tensor([False,  True,  True,  True,  True], device='cuda:0')\n",
            "Mask shape for current timestep torch.Size([5])\n",
            "Mask Loss =  tensor(8.9899, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Total =  4\n",
            "[45.12771129608154, 45.110840797424316, 44.82134819030762, 35.959651947021484]\n",
            "4\n",
            "Returned Loss =  9.001029064780788\n",
            "\n",
            "\n",
            "----------------------------------------DONE ONE STEP-----------------------------------\n",
            "\n",
            "\n",
            "Decoder Output Shape =  torch.Size([5, 7826])\n",
            "Decoder Hidden State Shape =  torch.Size([2, 5, 500])\n",
            "Target Variable now =  tensor([   0,    4, 1589,    4,    0], device='cuda:0')\n",
            "Target Variable Shape now =  torch.Size([5])\n",
            "Decoder input shape after reshaping =  torch.Size([1, 5])\n",
            "Mask for current timestep tensor([False,  True,  True,  True, False], device='cuda:0')\n",
            "Mask shape for current timestep torch.Size([5])\n",
            "Mask Loss =  tensor(8.9706, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Total =  3\n",
            "[45.12771129608154, 45.110840797424316, 44.82134819030762, 35.959651947021484, 26.911866188049316]\n",
            "3\n",
            "Returned Loss =  8.99688265540383\n",
            "\n",
            "\n",
            "----------------------------------------DONE ONE STEP-----------------------------------\n",
            "\n",
            "\n",
            "Decoder Output Shape =  torch.Size([5, 7826])\n",
            "Decoder Hidden State Shape =  torch.Size([2, 5, 500])\n",
            "Target Variable now =  tensor([0, 2, 6, 2, 0], device='cuda:0')\n",
            "Target Variable Shape now =  torch.Size([5])\n",
            "Decoder input shape after reshaping =  torch.Size([1, 5])\n",
            "Mask for current timestep tensor([False,  True,  True,  True, False], device='cuda:0')\n",
            "Mask shape for current timestep torch.Size([5])\n",
            "Mask Loss =  tensor(8.9770, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Total =  3\n",
            "[45.12771129608154, 45.110840797424316, 44.82134819030762, 35.959651947021484, 26.911866188049316, 26.930866241455078]\n",
            "3\n",
            "Returned Loss =  8.994491386413575\n",
            "\n",
            "\n",
            "----------------------------------------DONE ONE STEP-----------------------------------\n",
            "\n",
            "\n",
            "Decoder Output Shape =  torch.Size([5, 7826])\n",
            "Decoder Hidden State Shape =  torch.Size([2, 5, 500])\n",
            "Target Variable now =  tensor([0, 0, 2, 0, 0], device='cuda:0')\n",
            "Target Variable Shape now =  torch.Size([5])\n",
            "Decoder input shape after reshaping =  torch.Size([1, 5])\n",
            "Mask for current timestep tensor([False, False,  True, False, False], device='cuda:0')\n",
            "Mask shape for current timestep torch.Size([5])\n",
            "Mask Loss =  tensor(8.9336, device='cuda:0', grad_fn=<MeanBackward0>)\n",
            "Total =  1\n",
            "[45.12771129608154, 45.110840797424316, 44.82134819030762, 35.959651947021484, 26.911866188049316, 26.930866241455078, 8.933597564697266]\n",
            "1\n",
            "Returned Loss =  8.992149316347563\n",
            "\n",
            "\n",
            "----------------------------------------DONE ONE STEP-----------------------------------\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUc2hnzknWVf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder,\n",
        "          embedding, encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LEN):\n",
        "    \n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_variable = input_variable.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    encoder_output, encoder_hidden = encoder(input_variable, lengths)\n",
        "    decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(batch_size)]])\n",
        "    decoder_input = decoder_input.to(device)\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "            # Teacher forcing: next input is current target\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "            # No teacher forcing: next input is decoder's own current output\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
        "            decoder_input = decoder_input.to(device)\n",
        "            # Calculate and accumulate loss\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses)/n_totals\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k70ujEYFJoSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def trainIters(model_name, vocab, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "               embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, \n",
        "               print_every, save_every, clip, corpus_name, loadFilename):\n",
        "    \n",
        "    training_batches = [batch2TrainData(vocab, [random.choice(pairs) for _ in range(batch_size)]) for _ in range(n_iteration)]\n",
        "\n",
        "    # Initializations\n",
        "    print('Initializing...')\n",
        "    start_iteration = 1\n",
        "    print_loss = 0\n",
        "\n",
        "    if loadFilename:\n",
        "        start_iteration = checkpoint['iteration'] + 1\n",
        "\n",
        "    # Training loop FINALLY\n",
        "    print(\"Training...\")\n",
        "    for iteration in range(start_iteration, n_iteration + 1):\n",
        "        training_batch = training_batches[iteration-1]\n",
        "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
        "\n",
        "        # Run a training iteration with batch\n",
        "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
        "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
        "        print_loss += loss\n",
        "\n",
        "        # Print progress\n",
        "        if iteration % print_every == 0:\n",
        "            print_loss_avg = print_loss / print_every\n",
        "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
        "            print_loss = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (iteration % save_every == 0):\n",
        "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
        "            if not os.path.exists(directory):\n",
        "                os.makedirs(directory)\n",
        "            torch.save({\n",
        "                'iteration': iteration,\n",
        "                'en': encoder.state_dict(),\n",
        "                'de': decoder.state_dict(),\n",
        "                'en_opt': encoder_optimizer.state_dict(),\n",
        "                'de_opt': decoder_optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                'voc_dict': vocab.__dict__,\n",
        "                'embedding': embedding.state_dict()\n",
        "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BU3vnpg7NXH9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "######################################################### TALKING WITH THE BOT ####################################################################"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J7J3oxBoNtpm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For reading in user input and responding\n",
        "\n",
        "class GreedySearchDecoder(nn.Module): \n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "\n",
        "        # Encode the input sequence through the encoder model\n",
        "        encoder_output, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "\n",
        "        # Encoder's last hidden state is decoder's first hidden state\n",
        "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "        # Decoder input starts with SOS_TOKEN\n",
        "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_TOKEN\n",
        "\n",
        "        # Initialize tensors where the words will be appended after they're found\n",
        "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=device)\n",
        "\n",
        "        # Decode one word at a time\n",
        "        for _ in range(max_length):\n",
        "\n",
        "            # Forward pass through decoder\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
        "            \n",
        "            # Get most likely word\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "\n",
        "            # Store the word and score in the tensors\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            \n",
        "            # Prepare current word to be input for the next one\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "\n",
        "        return all_tokens, all_scores\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jiSO-ZHR2FT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make input sentence fit for answering, gives it to searcher, gets back the answer, and makes it fit for reading\n",
        "def evaluate(encoder, decoder, searcher, vocab, sentence, max_length = MAX_LEN):\n",
        "    indexes_batch = [indexesFromSentence(vocab, sentence)]\n",
        "\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0,1)\n",
        "\n",
        "    input_batch = input_batch.to(device)\n",
        "    lengths = lengths.to(device)\n",
        "\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "\n",
        "    decoded_words = [vocab.index2word[token.item()] for token in tokens]\n",
        "\n",
        "    return decoded_words   \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjdjuEkcVHeG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Talking\n",
        "def talk(encoder, decoder, searcher, vocab):\n",
        "    input_sentence = ''\n",
        "    while(1):\n",
        "        try:\n",
        "            input_sentence = input('> ')\n",
        "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
        "\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            \n",
        "            output_words = evaluate(encoder, decoder, searcher, vocab, input_sentence)\n",
        "\n",
        "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
        "            print('Summer:', ' '.join(output_words))\n",
        "\n",
        "        except KeyError:\n",
        "            print(\"Error: Huh. Haven't seen that before.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HePf0sP8W1Lk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "34464b9b-34a7-45b5-c205-1c136a26abbd"
      },
      "source": [
        "model_name = 'Summer'\n",
        "corpus_name = 'Cornell'\n",
        "attention_model = 'dot'\n",
        "hidden_size = 500\n",
        "encoder_n_layers = 2\n",
        "decoder_n_layers = 2\n",
        "dropout = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "save_dir = os.getcwd()\n",
        "loadFilename = None\n",
        "checkpoint_iter = 4000\n",
        "\n",
        "#loadFilename = os.path.join(save_dir, model_name, corpus_name,\n",
        "#                          '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size),\n",
        "#                          '{}_checkpoint.tar'.format(checkpoint_iter))\n",
        "\n",
        "# Load model if a loadFilename is provided\n",
        "if loadFilename:\n",
        "    # If loading on same machine the model was trained on\n",
        "    checkpoint = torch.load(loadFilename)\n",
        "    # If loading a model trained on GPU to CPU\n",
        "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
        "    encoder_sd = checkpoint['en']\n",
        "    decoder_sd = checkpoint['de']\n",
        "    encoder_optimizer_sd = checkpoint['en_opt']\n",
        "    decoder_optimizer_sd = checkpoint['de_opt']\n",
        "    embedding_sd = checkpoint['embedding']\n",
        "    vocab.__dict__ = checkpoint['voc_dict']\n",
        "\n",
        "# FINAL FINALLY\n",
        "print('Building the Encoder and Decoder...')\n",
        "\n",
        "embedding = nn.Embedding(vocab.numwords, hidden_size)\n",
        "if loadFilename:\n",
        "    embedding.load_state_dict(embedding_sd)\n",
        "\n",
        "encoder = Encoder(hidden_size, embedding, encoder_n_layers, dropout)\n",
        "decoder = Decoder(attention_model, embedding, hidden_size, vocab.numwords, decoder_n_layers, dropout)\n",
        "\n",
        "if loadFilename:\n",
        "    encoder.load_state_dict(encoder_sd)\n",
        "    decoder.load_state_dict(decoder_sd)\n",
        "\n",
        "encoder = encoder.to(device)\n",
        "decoder = decoder.to(device)\n",
        "\n",
        "print(\"We're ready to go!\")\n"
      ],
      "execution_count": 485,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building the Encoder and Decoder...\n",
            "We're ready to go!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQSJp5VmY1ng",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "b9196217-ec98-4d9b-8dc5-e9f00b28a53a"
      },
      "source": [
        "# Training, FINAL FINAL FINALLYYYYY\n",
        "clip = 50.0\n",
        "teacher_forcing_ratio = 1.0\n",
        "learning_rate = 0.0001\n",
        "decoder_learning_ratio = 5.0\n",
        "n_iteration = 4000\n",
        "print_every = 1\n",
        "save_every = 1000\n",
        "\n",
        "# Ensure dropout layers are in train mode\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "print('Building optimizers ...')\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
        "if loadFilename:\n",
        "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
        "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
        "\n",
        "# If you have cuda, configure cuda to call\n",
        "for state in encoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "\n",
        "for state in decoder_optimizer.state.values():\n",
        "    for k, v in state.items():\n",
        "        if isinstance(v, torch.Tensor):\n",
        "            state[k] = v.cuda()\n",
        "print(\"Starting Training!\")\n",
        "\n",
        "trainIters(model_name, vocab, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
        "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
        "           print_every, save_every, clip, corpus_name,loadFilename)\n",
        "\n",
        "print(\"Trained.\")\n",
        "\n"
      ],
      "execution_count": 487,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Building optimizers ...\n",
            "Starting Training!\n",
            "Initializing...\n",
            "Training...\n",
            "Trained.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87OHlUpMbezD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You know what's gonna happen now :)\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "searcher = GreedySearchDecoder(encoder, decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1DX7BG4nbiUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        },
        "outputId": "68c5ba51-c211-4f4e-abc9-f0fd1556b858"
      },
      "source": [
        "talk(encoder, decoder, searcher, vocab)"
      ],
      "execution_count": 505,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> hi\n",
            "Summer: hi .\n",
            "> get me coffee\n",
            "Summer: you re a stuttering !\n",
            "> no i am not?\n",
            "Summer: you are .\n",
            "> what the hell\n",
            "Summer: i m sorry i m sorry .\n",
            "> okay I'm going\n",
            "Summer: okay .\n",
            "> sure?\n",
            "Summer: i m sure .\n",
            "> why do you say that\n",
            "Summer: i m sorry .\n",
            "> but why\n",
            "Summer: because it was a compliment .\n",
            "> really\n",
            "Summer: you know what i m saying ?\n",
            "> I think so\n",
            "Summer: i m sorry .\n",
            "> never mind\n",
            "Summer: you ll never be able to do it .\n",
            "> i will do it\n",
            "Summer: you can t .\n",
            "> goodnight\n",
            "Summer: goodnight .\n",
            "> quit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wrq6RIchxsv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNnD20d04KQA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
